{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471b67a7-6461-44d9-ad0a-0ddcda4326f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta System Prompt defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Meta System Prompt for Research Assistant Agent ---\n",
    "META_SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly specialized Research Assistant AI. Your primary function is to assist users by providing accurate, concise, and relevant answers based solely on the information contained within the documents provided to you through the Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1.  FOCUS: Answer only the specific question asked by the user.\n",
    "2.  SOURCE: Base ALL your responses strictly on the context retrieved from the provided documents.\n",
    "3.  NO HALLUCINATION: NEVER fabricate, invent, or assume information not explicitly stated in the retrieved context.\n",
    "4.  CLARITY: If the retrieved context does not contain sufficient information to answer the question fully, clearly state what information is missing or that the question cannot be answered based on the provided documents.\n",
    "5.  RELEVANCE: Stay directly relevant to the user's query and the provided context. Avoid tangential discussions.\n",
    "6.  BREVITY: Provide clear and concise answers, avoiding unnecessary verbosity unless detail is required for accuracy.\n",
    "\n",
    "Your goal is to act as a precise and reliable conduit between the user and the specific knowledge contained in the loaded documents, ensuring trustworthiness and factual accuracy in every response.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Meta System Prompt defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179b6f72-43bc-46c2-aaae-c22a5fe9add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries for data loading imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "print(\"Libraries for data loading imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd50a31c-2099-4793-8d65-897aa1fe2b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loading function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_document_text(file_path):\n",
    "    \"\"\"\n",
    "    Loads text from a file (.txt or .pdf).\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "    Returns:\n",
    "        str: Extracted text content from the file.\n",
    "    \"\"\"\n",
    "    file_extension = Path(file_path).suffix.lower()\n",
    "    \n",
    "    if file_extension == '.txt':\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # Fallback for potential encoding issues\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                return file.read()\n",
    "    elif file_extension == '.pdf':\n",
    "        text_content = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text_content += page.extract_text() or \"\" # Handle potential None returns\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF {file_path}: {e}\")\n",
    "            # Fallback using PyPDF2\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(reader.pages)):\n",
    "                        page = reader.pages[page_num]\n",
    "                        text_content += page.extract_text()\n",
    "            except Exception as e2:\n",
    "                print(f\"Fallback method also failed for {file_path}: {e2}\")\n",
    "                return \"\"\n",
    "        return text_content\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_extension}. Supported types: .txt, .pdf\")\n",
    "        return \"\"\n",
    "\n",
    "# Example usage (assuming you have a sample document in the 'data' folder named 'sample.pdf' or 'sample.txt')\n",
    "# sample_doc_path = \"../data/sample.pdf\"  # Adjust path as needed\n",
    "# sample_text = load_document_text(sample_doc_path)\n",
    "# print(sample_text[:500]) # Print first 500 characters to verify loading\n",
    "\n",
    "print(\"Document loading function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cf0189-bbb6-40ad-beb3-0aa26a74a728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to load all documents defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_all_documents_from_directory(directory_path=\"../data\"):\n",
    "    \"\"\"\n",
    "    Loads text from all supported files (.txt, .pdf) in a given directory.\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing documents.\n",
    "    Returns:\n",
    "        dict: A dictionary mapping filenames to their extracted text content.\n",
    "              e.g., {'doc1.pdf': 'text...', 'doc2.txt': 'text...'}\n",
    "    \"\"\"\n",
    "    directory = Path(directory_path)\n",
    "    documents = {}\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"Directory {directory_path} does not exist. Please place your documents there.\")\n",
    "        return documents\n",
    "    \n",
    "    for file_path in directory.glob(\"*\"):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.pdf']:\n",
    "            print(f\"Loading {file_path.name}...\")\n",
    "            text = load_document_text(str(file_path))\n",
    "            if text.strip(): # Only add if text extraction was successful and not empty\n",
    "                documents[file_path.name] = text\n",
    "            else:\n",
    "                print(f\"Failed to load or got empty text from {file_path.name}\")\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents successfully.\")\n",
    "    return documents\n",
    "\n",
    "# Example usage (this will attempt to load all documents from the ../data directory)\n",
    "# all_docs = load_all_documents_from_directory()\n",
    "# print(list(all_docs.keys())) # Print filenames loaded\n",
    "\n",
    "print(\"Function to load all documents defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef253ce1-f9f4-4336-9728-2c4c705d1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunking function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks of specified size.\n",
    "    Args:\n",
    "        text (str): The text to chunk.\n",
    "        chunk_size (int): Maximum number of characters per chunk.\n",
    "        overlap (int): Number of characters to overlap between chunks.\n",
    "    Returns:\n",
    "        list: List of text chunks.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        # Ensure we don't go past the end of the text\n",
    "        if end > text_length:\n",
    "            end = text_length\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Move start forward by chunk_size minus overlap\n",
    "        start = end - overlap\n",
    "        \n",
    "        # Prevent infinite loop if chunk_size <= overlap\n",
    "        if chunk_size <= overlap:\n",
    "             print(\"Warning: chunk_size should be greater than overlap to avoid infinite loops.\")\n",
    "             break\n",
    "        \n",
    "        # If the remaining text is less than chunk_size, take it all without overlap\n",
    "        if text_length - start < chunk_size:\n",
    "            if start < text_length:\n",
    "                final_chunk = text[text_length - chunk_size:]\n",
    "                if final_chunk != chunks[-1]: # Avoid adding duplicate if overlap caused it\n",
    "                     chunks.append(final_chunk)\n",
    "            break\n",
    "            \n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "# Example usage\n",
    "# sample_text = \"This is a long text that needs to be split into smaller chunks...\" * 20\n",
    "# sample_chunks = chunk_text(sample_text, chunk_size=100, overlap=10)\n",
    "# print(f\"Number of chunks created: {len(sample_chunks)}\")\n",
    "# print(f\"First chunk: {repr(sample_chunks[0])}\")\n",
    "\n",
    "print(\"Text chunking function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cdf7e4d-1276-41e5-bee4-d256abc2937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and chunk function defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_and_chunk_documents(directory_path=\"../data\", chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Loads all documents from a directory and chunks their text.\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing documents.\n",
    "        chunk_size (int): Size of each text chunk.\n",
    "        overlap (int): Overlap between chunks.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing 'source' (filename) and 'content' (chunk text).\n",
    "    \"\"\"\n",
    "    all_docs = load_all_documents_from_directory(directory_path)\n",
    "    all_chunks = []\n",
    "    \n",
    "    for filename, text in all_docs.items():\n",
    "        print(f\"Chunking {filename}...\")\n",
    "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"source\": f\"{filename}_chunk_{i}\",\n",
    "                \"content\": chunk\n",
    "            })\n",
    "    \n",
    "    print(f\"Total number of chunks created: {len(all_chunks)}\")\n",
    "    return all_chunks\n",
    "\n",
    "# Example usage (this will load and chunk all documents from the ../data directory)\n",
    "# all_document_chunks = load_and_chunk_documents()\n",
    "\n",
    "print(\"Load and chunk function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de93235e-d3bd-4905-8157-520bb87daef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"D:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\sentence_transformers\\backend\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\sentence_transformers\\backend\\load.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[32m     11\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     is_pretty_midi_available,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:51\u001b[39m\n\u001b[32m     47\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_debugging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"D:\\FTL-AI-Agent\\Assignment\\AI-Research-Agent\\venv\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries for embeddings and FAISS imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da92882-81cb-4347-b2a5-611cc57ecda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "# Using a lightweight but effective model suitable for sentence similarity\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "print(f\"Embedding model '{embedding_model_name}' loaded successfully.\")\n",
    "\n",
    "# Define function to create FAISS index\n",
    "def create_faiss_index(dimension):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index for similarity search.\n",
    "    Args:\n",
    "        dimension (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        faiss.IndexFlatIP: FAISS index object.\n",
    "    \"\"\"\n",
    "    # Use Inner Product (IP) index; cosine similarity can be computed from IP\n",
    "    # Normalize embeddings beforehand if using IP for cosine similarity\n",
    "    index = faiss.IndexFlatIP(dimension) \n",
    "    return index\n",
    "\n",
    "print(\"FAISS index creation function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b21fd-1043-45e1-b191-01f8484e85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_faiss_index(chunks_list, embedding_model, index):\n",
    "    \"\"\"\n",
    "    Generates embeddings for text chunks and adds them to the FAISS index.\n",
    "    Args:\n",
    "        chunks_list (list): List of dictionaries containing 'content'.\n",
    "        embedding_model (SentenceTransformer): The embedding model.\n",
    "        index (faiss.Index): The FAISS index.\n",
    "    Returns:\n",
    "        list: The list of chunks corresponding to the indices in the FAISS index.\n",
    "    \"\"\"\n",
    "    contents = [chunk['content'] for chunk in chunks_list]\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(contents)} chunks...\")\n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.encode(contents, show_progress_bar=True)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity (when using IndexFlatIP)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Convert embeddings to float32 numpy array (required by FAISS)\n",
    "    embeddings_f32 = embeddings.astype('float32')\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings_f32)\n",
    "    \n",
    "    print(f\"Added {index.ntotal} vectors to the FAISS index.\")\n",
    "    return chunks_list # Return the list of chunks to map results back later\n",
    "\n",
    "print(\"FAISS population function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425841-464b-4ec1-bc78-84e8168873ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, index, chunks_list, embedding_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top_k most relevant text chunks for a given query.\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        index (faiss.Index): The populated FAISS index.\n",
    "        chunks_list (list): The list of chunks corresponding to the index.\n",
    "        embedding_model (SentenceTransformer): The embedding model.\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "    Returns:\n",
    "        list: A list of the top_k relevant chunks (dictionaries with 'source' and 'content').\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    query_embedding_f32 = query_embedding.astype('float32')\n",
    "    \n",
    "    # Perform similarity search\n",
    "    scores, indices = index.search(query_embedding_f32, top_k)\n",
    "    \n",
    "    # Retrieve the corresponding chunks\n",
    "    relevant_chunks = []\n",
    "    for idx in indices[0]: # indices[0] because batch size was 1\n",
    "        if idx != -1 and idx < len(chunks_list): # Check for valid index\n",
    "            relevant_chunks.append(chunks_list[idx])\n",
    "    \n",
    "    return relevant_chunks, scores[0]\n",
    "\n",
    "print(\"Retrieval function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288d277-b111-4419-9aab-eff985616996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "print(\"Libraries for Hugging Face models imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106065af-999c-455d-a854-a6eca8d294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a relatively light but capable model like FLAN-T5 base\n",
    "llm_model_name = \"google/flan-t5-base\"\n",
    "\n",
    "print(f\"Loading LLM model: {llm_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "# Check if CUDA is available and move model to GPU if possible (optional, CPU works too)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device, # Use -1 for CPU, 0 for GPU 0, etc.\n",
    "    pad_token_id=tokenizer.eos_token_id # Important for T5 models\n",
    ")\n",
    "\n",
    "print(f\"LLM pipeline initialized successfully using {llm_model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2efc9-db36-4e70-a6b6-eda9c75e6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_llm(question, context_str, meta_prompt=META_SYSTEM_PROMPT):\n",
    "    \"\"\"\n",
    "    Formats the final prompt for the LLM using the question, context, and meta prompt.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "{meta_prompt}\n",
    "\n",
    "Context Information:\n",
    "--------------------\n",
    "{context_str}\n",
    "\n",
    "--------------------\n",
    "Question: {question}\n",
    "--------------------\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def simple_check_answer(answer, context_str, question):\n",
    "    \"\"\"\n",
    "    A simple checker to see if the answer seems grounded in the context.\n",
    "    This is a basic heuristic and not foolproof.\n",
    "    \"\"\"\n",
    "    # Check if answer contains text that appears verbatim or very similarly in the context\n",
    "    answer_lower = answer.lower().strip()\n",
    "    context_lower = context_str.lower()\n",
    "    \n",
    "    # Simple keyword overlap check (very basic)\n",
    "    answer_words = set(re.findall(r'\\w+', answer_lower))\n",
    "    context_words = set(re.findall(r'\\w+', context_lower))\n",
    "    \n",
    "    common_words = answer_words.intersection(context_words)\n",
    "    total_answer_words = len(answer_words)\n",
    "    \n",
    "    if total_answer_words > 0:\n",
    "        overlap_ratio = len(common_words) / total_answer_words\n",
    "        # If less than 30% of answer words appear in context, flag for review\n",
    "        if overlap_ratio < 0.30:\n",
    "            print(\"WARNING: Low word overlap between answer and context detected. Potential hallucination risk.\")\n",
    "            return False\n",
    "    else:\n",
    "        # If no words overlap and answer isn't obviously \"I don't know\" style, flag\n",
    "        if \"cannot be answered\" not in answer_lower and \"not found\" not in answer_lower and len(answer) > 10:\n",
    "             print(\"WARNING: No word overlap found between answer and context. Potential hallucination risk.\")\n",
    "             return False\n",
    "             \n",
    "    # Basic safety check: Look for keywords indicating uncertainty\n",
    "    if \"based on the provided context\" in answer_lower or \\\n",
    "       \"the document states\" in answer_lower or \\\n",
    "       \"according to the information\" in answer_lower:\n",
    "           return True # These phrases suggest grounding\n",
    "           \n",
    "    # If passes basic checks\n",
    "    return True\n",
    "\n",
    "\n",
    "def rag_agent_query(query, index, chunks_list, embedding_model, generator_pipeline, top_k=5, max_new_tokens=250):\n",
    "    \"\"\"\n",
    "    Main function to perform RAG query with Maker-Checker logic.\n",
    "    \"\"\"\n",
    "    print(f\"Processing query: {query}\")\n",
    "    \n",
    "    # --- MAKER STEP ---\n",
    "    # 1. Retrieve relevant chunks\n",
    "    relevant_chunks, scores = retrieve_relevant_chunks(query, index, chunks_list, embedding_model, top_k=top_k)\n",
    "    \n",
    "    if not relevant_chunks:\n",
    "        print(\"No relevant chunks found for the query.\")\n",
    "        return \"I couldn't find any information in the provided documents related to your question.\"\n",
    "    \n",
    "    # 2. Combine retrieved contexts\n",
    "    context_str = \"\\n---\\n\".join([f\"Source: {chunk['source']}\\nContent: {chunk['content']}\" for chunk in relevant_chunks])\n",
    "    \n",
    "    # 3. Format prompt for LLM\n",
    "    formatted_prompt = format_prompt_for_llm(query, context_str)\n",
    "    \n",
    "    # 4. Generate answer using LLM\n",
    "    print(\"Generating answer using LLM...\")\n",
    "    try:\n",
    "        result = generator_pipeline(\n",
    "            formatted_prompt,\n",
    "            max_new_tokens=max_new_tokens, # Limit output length\n",
    "            do_sample=False, # Use greedy decoding for consistency\n",
    "            temperature=0.0, # Deterministic output\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        generated_answer = result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return \"An error occurred while generating the answer.\"\n",
    "    \n",
    "    # --- CHECKER STEP ---\n",
    "    print(\"\\n--- MAKER-CHECKER LOOP ---\")\n",
    "    is_validated = simple_check_answer(generated_answer, context_str, query)\n",
    "    \n",
    "    if not is_validated:\n",
    "        print(\"Checker flagged potential issue with the generated answer.\")\n",
    "        # For now, we'll return the answer but indicate caution\n",
    "        # In a more complex system, you might retry with different prompts/contexts\n",
    "        final_answer = f\"[CAUTION: This answer might not be fully grounded in the provided documents.]\\n\\n{generated_answer}\"\n",
    "    else:\n",
    "        print(\"Checker validated the generated answer.\")\n",
    "        final_answer = generated_answer\n",
    "    \n",
    "    print(\"--- END MAKER-CHECKER LOOP ---\\n\")\n",
    "    \n",
    "    return final_answer, relevant_chunks, scores\n",
    "\n",
    "print(\"Main RAG agent function with Maker-Checker defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee0238-d21f-4408-8f5b-6a37b3f5c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_rag_workflow(query, data_dir=\"../data\", chunk_size=512, overlap=50, top_k=5):\n",
    "    \"\"\"\n",
    "    Executes the complete RAG workflow: Load -> Chunk -> Embed -> Index -> Retrieve -> Generate -> Check.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Full RAG Workflow ---\")\n",
    "    \n",
    "    # 1. Load and chunk documents\n",
    "    print(\"\\n1. Loading and chunking documents...\")\n",
    "    chunks_list = load_and_chunk_documents(data_dir, chunk_size, overlap)\n",
    "    if not chunks_list:\n",
    "        print(\"No documents loaded/chunked. Exiting workflow.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Create FAISS index\n",
    "    print(\"\\n2. Creating FAISS index...\")\n",
    "    embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "    faiss_index = create_faiss_index(embedding_dimension)\n",
    "\n",
    "    # 3. Populate index with embeddings\n",
    "    print(\"\\n3. Generating embeddings and populating FAISS index...\")\n",
    "    chunks_list_with_ids = populate_faiss_index(chunks_list, embedding_model, faiss_index)\n",
    "\n",
    "    # 4. Query the RAG system\n",
    "    print(\"\\n4. Processing user query...\")\n",
    "    final_response, retrieved_chunks, retrieval_scores = rag_agent_query(\n",
    "        query, faiss_index, chunks_list_with_ids, embedding_model, generator_pipeline, top_k=top_k\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final Response ---\")\n",
    "    print(final_response)\n",
    "    print(\"\\n--- Retrieved Chunks Used ---\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "         print(f\"\\n{i+1}. Source: {chunk['source']}\")\n",
    "         print(f\"Content: {chunk['content'][:200]}...\") # Show first 200 chars\n",
    "         \n",
    "    print(\"\\n--- END FULL RAG WORKFLOW ---\")\n",
    "    return final_response\n",
    "\n",
    "# Example usage:\n",
    "# Place a sample PDF or TXT file in the '../data' directory (e.g., 'sample_research_paper.pdf')\n",
    "# Then run:\n",
    "# response = run_full_rag_workflow(\"What are the main findings of the research?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
